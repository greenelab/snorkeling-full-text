{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Discriminator Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is not designed to be executed. Only to hold code in a similar format. Due to my lab computer not have a GPU the majority of training is on UPenn's computer cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n",
    "from sklearn.metrics import auc, precision_recall_curve, roc_curve\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LF_SAMPLE_SIZE = 3\n",
    "NUM_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(\n",
    "    description=\"Train BioBert as Discriminator Model. Use these two commands to run the model.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--training_marginals_folder\", help=\"The folder to access the training models.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--edge_type\",\n",
    "    help=\"The edge type to use to predict the sentences. Valid options are DaG, CbG, CtD, GiG\",\n",
    ")\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.edge_type.lower() == \"dag\":\n",
    "    edge_prediction = \"DaG\"\n",
    "    curated_label = \"curated_dsh\"\n",
    "\n",
    "    validation_file = \"DaG/training_sen/dg_dev_test_encoded_lemmas.tsv\"\n",
    "\n",
    "    training_file = \"DaG/training_sen/train_dg_abstract_encoded_lemmas.tsv\"\n",
    "    training_labels_folder = args.training_marginals_folder\n",
    "\n",
    "    entity_replace_one = \"DISEASE_ENTITY\"\n",
    "    one_replace = \"@DISEASE$\"\n",
    "    entity_replace_two = \"GENE_ENTITY\"\n",
    "    two_replace = \"@GENE$\"\n",
    "\n",
    "if args.edge_type.lower() == \"ctd\":\n",
    "    edge_prediction = \"CtD\"\n",
    "    curated_label = \"curated_ctd\"\n",
    "\n",
    "    validation_file = \"CtD/training_sen/cd_dev_test_encoded_lemmas.tsv\"\n",
    "\n",
    "    training_file = \"CtD/training_sen/train_cd_abstract_encoded_lemmas.tsv\"\n",
    "    training_labels_folder = args.training_marginals_folder\n",
    "\n",
    "    entity_replace_one = \"COMPOUND_ENTITY\"\n",
    "    one_replace = \"@CHEMICAL$\"\n",
    "    entity_replace_two = \"DISEASE_ENTITY\"\n",
    "    two_replace = \"@DISEASE$\"\n",
    "\n",
    "if args.edge_type.lower() == \"cbg\":\n",
    "    edge_prediction = \"CbG\"\n",
    "    curated_label = \"curated_cbg\"\n",
    "\n",
    "    validation_file = \"CbG/training_sen/cg_dev_test_encoded_lemmas.tsv\"\n",
    "\n",
    "    training_file = \"CbG/training_sen/train_cg_abstract_encoded_lemmas.tsv\"\n",
    "    training_labels_folder = args.training_marginals_folder\n",
    "\n",
    "    entity_replace_one = \"COMPOUND_ENTITY\"\n",
    "    one_replace = \"@CHEMICAL$\"\n",
    "    entity_replace_two = \"GENE_ENTITY\"\n",
    "    two_replace = \"@GENE$\"\n",
    "\n",
    "if args.edge_type.lower() == \"gig\":\n",
    "    edge_prediction = \"GiG\"\n",
    "    curated_label = \"curated_gig\"\n",
    "\n",
    "    validation_file = \"GiG/training_sen/gg_dev_test_encoded_lemmas.tsv\"\n",
    "\n",
    "    training_file = \"GiG/training_sen/train_gg_abstract_encoded_lemmas.tsv\"\n",
    "    training_labels_folder = args.training_marginals_folder\n",
    "\n",
    "    entity_replace_one = \"GENE1_ENTITY\"\n",
    "    one_replace = \"@GENE$\"\n",
    "    entity_replace_two = \"GENE2_ENTITY\"\n",
    "    two_replace = \"@GENE$\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"../biobert-base-cased-v1.1\", local_files_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Validation\n",
    "validation_data = pd.read_csv(validation_file, sep=\"\\t\").rename(\n",
    "    index=str, columns={\"split\": \"dataset\", curated_label: \"labels\"}\n",
    ")\n",
    "dev_split_id = validation_data.dataset.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset = Dataset.from_pandas(\n",
    "    validation_data.query(f\"dataset=={dev_split_id}\")[[\"parsed_lemmas\", \"labels\"]]\n",
    ")\n",
    "\n",
    "validation_dataset = validation_dataset.map(\n",
    "    lambda x: tokenizer(\n",
    "        \" \".join(\n",
    "            x[\"parsed_lemmas\"]\n",
    "            .replace(entity_replace_one, one_replace)\n",
    "            .replace(entity_replace_two, two_replace)\n",
    "            .split(\"|\")\n",
    "        ),\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=100,\n",
    "        truncation=True,\n",
    "    ),\n",
    "    remove_columns=[\"parsed_lemmas\"],\n",
    ")\n",
    "\n",
    "validation_dataset_pt = DataLoader(validation_dataset, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training\n",
    "if edge_prediction == \"GiG\":\n",
    "    training_data = pd.read_csv(training_file, sep=\"\\t\").assign(\n",
    "        length=lambda x: x.parsed_lemmas.apply(lambda y: y.count(\"|\")),\n",
    "        entity_length=lambda x: x.parsed_lemmas.apply(\n",
    "            lambda y: y.split(\"|\").index(entity_replace_one)\n",
    "            - y.split(\"|\").index(entity_replace_two)\n",
    "            if \"GENE2_ENTITY\" in y\n",
    "            else 0\n",
    "        ),\n",
    "    )\n",
    "else:\n",
    "    training_data = pd.read_csv(training_file, sep=\"\\t\").assign(\n",
    "        length=lambda x: x.parsed_lemmas.apply(lambda y: y.count(\"|\"))\n",
    "    )\n",
    "\n",
    "marginal_files = sorted(list(Path(training_labels_folder).rglob(\"*tsv*\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for training_marginals in marginal_files:\n",
    "    training_marginals_df = pd.read_csv(str(training_marginals), sep=\"\\t\")\n",
    "\n",
    "    np.random.seed(100)\n",
    "    unique_lf_num = sorted(training_marginals_df.lf_num.unique())\n",
    "    lf_size = list(range(training_marginals_df.iteration.max() + 1))\n",
    "    lf_sample_selection = np.random.choice(lf_size, size=LF_SAMPLE_SIZE)\n",
    "    print(lf_sample_selection)\n",
    "\n",
    "    match = re.search(\n",
    "        f\"(\\w+)_predicts_{edge_prediction}\",  # noqa: W605\n",
    "        str(training_marginals),\n",
    "        flags=re.I,  # noqa: W605\n",
    "    )\n",
    "\n",
    "    if match is None:\n",
    "        edge_source = f\"{edge_prediction}_baseline\"\n",
    "\n",
    "    else:\n",
    "        edge_source = match.group(1)\n",
    "        edge_source = edge_source[0:2].capitalize() + edge_source[2].upper()\n",
    "\n",
    "    if not Path(f\"{edge_prediction}/{edge_source}\").exists():\n",
    "        Path(f\"{edge_prediction}/{edge_source}\").mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    for lf_group in unique_lf_num:\n",
    "        print(lf_group)\n",
    "\n",
    "        for accepted_iteration in lf_sample_selection:\n",
    "\n",
    "            if Path(\n",
    "                f\"{edge_prediction}/{edge_source}/biobert_{lf_group}_{accepted_iteration}.model\"\n",
    "            ).exists():\n",
    "                continue\n",
    "\n",
    "            filtered_group_marginal_df = training_marginals_df.query(\n",
    "                f\"iteration=={accepted_iteration}&lf_num=={lf_group}\"\n",
    "            )\n",
    "\n",
    "            # grab the marginals and set up the dataset loader\n",
    "            # filtered_group_marginal_df.columns = sorted(list(filtered_group_marginal_df.columns))\n",
    "            training_marginals_mapper = dict(\n",
    "                zip(\n",
    "                    filtered_group_marginal_df[\"candidate\"],\n",
    "                    filtered_group_marginal_df[\"positive_marginals\"].apply(\n",
    "                        lambda x: 0 if x <= 0.5 else 1\n",
    "                    ),\n",
    "                )\n",
    "            )\n",
    "\n",
    "            if edge_prediction == \"GiG\":\n",
    "                training_dataset = Dataset.from_pandas(\n",
    "                    training_data.query(\n",
    "                        f\"candidate_id in {filtered_group_marginal_df.candidate.tolist()}\"\n",
    "                    )\n",
    "                    .query(\"length <= 100\")\n",
    "                    .query(\"abs(entity_length) >= 7\")[[\"candidate_id\", \"parsed_lemmas\"]]\n",
    "                )\n",
    "            else:\n",
    "                training_dataset = Dataset.from_pandas(\n",
    "                    training_data.query(\n",
    "                        f\"candidate_id in {filtered_group_marginal_df.candidate.tolist()}\"\n",
    "                    ).query(\"length <= 100\")[[\"candidate_id\", \"parsed_lemmas\"]]\n",
    "                )\n",
    "\n",
    "            training_dataset = training_dataset.map(\n",
    "                lambda x: tokenizer(\n",
    "                    \" \".join(\n",
    "                        x[\"parsed_lemmas\"]\n",
    "                        .replace(entity_replace_one, one_replace)\n",
    "                        .replace(entity_replace_two, two_replace)\n",
    "                        .split(\"|\")\n",
    "                    ),\n",
    "                    padding=\"max_length\",\n",
    "                    return_tensors=\"pt\",\n",
    "                    max_length=100,\n",
    "                    truncation=True,\n",
    "                ),\n",
    "                remove_columns=[\"parsed_lemmas\", \"__index_level_0__\"],\n",
    "            )\n",
    "\n",
    "            training_dataset = training_dataset.map(\n",
    "                lambda x: {\"labels\": training_marginals_mapper[x[\"candidate_id\"]]},\n",
    "                remove_columns=[\"candidate_id\"],\n",
    "            )\n",
    "\n",
    "            # here is the loader\n",
    "            # 128\n",
    "            training_dataset_pt = DataLoader(\n",
    "                training_dataset, batch_size=256, shuffle=True\n",
    "            )\n",
    "\n",
    "            # Load the Model\n",
    "            biobert = AutoModelForSequenceClassification.from_pretrained(\n",
    "                \"../biobert-base-cased-v1.1\", local_files_only=True, num_labels=2\n",
    "            )\n",
    "\n",
    "            # Freeze the body of the model\n",
    "            for param in biobert.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "            biobert = torch.nn.DataParallel(biobert)\n",
    "            biobert = biobert.cuda()\n",
    "            num_epochs = NUM_EPOCHS\n",
    "            learning_rate = 1e-3\n",
    "            optim = AdamW(biobert.parameters(), lr=learning_rate)\n",
    "            num_training_steps = num_epochs * len(training_dataset_pt)\n",
    "\n",
    "            # Load the writer\n",
    "            writer = SummaryWriter(f\"{edge_prediction}/{edge_source}/{lf_group}\")\n",
    "\n",
    "            num_steps = 0\n",
    "\n",
    "            # Run the training\n",
    "            for epoch in range(num_epochs + 1):\n",
    "\n",
    "                if epoch > 0 or True:\n",
    "                    biobert.train()\n",
    "                    for batch in tqdm.tqdm(training_dataset_pt):\n",
    "                        # batch.cuda()\n",
    "                        optim.zero_grad()\n",
    "                        attention_mask = (\n",
    "                            torch.stack(batch[\"attention_mask\"][0]).permute(1, 0).cuda()\n",
    "                        )\n",
    "                        input_ids = (\n",
    "                            torch.stack(batch[\"input_ids\"][0], dim=0)\n",
    "                            .permute(1, 0)\n",
    "                            .cuda()\n",
    "                        )\n",
    "                        output = biobert(\n",
    "                            attention_mask=attention_mask,\n",
    "                            input_ids=input_ids,\n",
    "                            labels=batch[\"labels\"].cuda(),\n",
    "                        )\n",
    "\n",
    "                        loss = output[0].mean()\n",
    "                        writer.add_scalar(\"Loss/train\", loss.item(), num_steps)\n",
    "\n",
    "                        loss.backward()\n",
    "\n",
    "                        ## Get the gradients for each model\n",
    "                        for name, param in biobert.named_parameters():\n",
    "                            if (param.requires_grad) and (\"bias\" not in name):\n",
    "                                writer.add_histogram(\n",
    "                                    f\"Layer/{name}\",\n",
    "                                    param.grad.abs().mean().item(),\n",
    "                                    num_steps,\n",
    "                                )\n",
    "\n",
    "                        optim.step()\n",
    "                        num_steps += 1\n",
    "\n",
    "                biobert.eval()\n",
    "                validation_loss = []\n",
    "                predictions = []\n",
    "                val_labels = []\n",
    "\n",
    "                for batch in validation_dataset_pt:\n",
    "                    attention_mask = (\n",
    "                        torch.stack(batch[\"attention_mask\"][0]).permute(1, 0).cuda()\n",
    "                    )\n",
    "                    input_ids = torch.stack(batch[\"input_ids\"][0]).permute(1, 0).cuda()\n",
    "                    output = biobert(\n",
    "                        attention_mask=attention_mask,\n",
    "                        input_ids=input_ids,\n",
    "                        labels=batch[\"labels\"].long().cuda(),\n",
    "                    )\n",
    "\n",
    "                    predictions.append(\n",
    "                        torch.nn.functional.softmax(output[1], dim=1)[:, 1].cpu()\n",
    "                    )\n",
    "                    val_labels.append(batch[\"labels\"].cpu())\n",
    "\n",
    "                    validation_loss.append(output[0].mean().item())\n",
    "\n",
    "                combined_labels = torch.cat(val_labels).numpy()\n",
    "                combined_predictions = torch.cat(predictions).detach().numpy()\n",
    "\n",
    "                # Tuning dataset loss\n",
    "                writer.add_scalar(\"Loss/tune\", np.mean(validation_loss), num_steps)\n",
    "\n",
    "                # AUROCo\n",
    "                fpr, tpr, _ = roc_curve(combined_labels, combined_predictions)\n",
    "                writer.add_scalar(\"Eval/AUROC\", auc(fpr, tpr), num_steps, num_steps)\n",
    "\n",
    "                # AUPRC/PRC\n",
    "                writer.add_pr_curve(\n",
    "                    \"Eval/PRC\", combined_labels, combined_predictions, num_steps\n",
    "                )\n",
    "\n",
    "                precision, recall, _ = precision_recall_curve(\n",
    "                    combined_labels, combined_predictions\n",
    "                )\n",
    "                current_model_auc = auc(recall, precision)\n",
    "                writer.add_scalar(\"Eval/AUPR\", current_model_auc, num_steps)\n",
    "\n",
    "            # Training finished for model save last predictions\n",
    "            biobert.module.save_pretrained(\n",
    "                f\"{edge_prediction}/{edge_source}/biobert_{lf_group}_{accepted_iteration}.model\"\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python [conda env:snorkeling_full_text]",
   "language": "python",
   "name": "conda-env-snorkeling_full_text-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
